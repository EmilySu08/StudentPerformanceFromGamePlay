{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c2d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125a6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "trlabels = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49e7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trlabels['session'] = trlabels.session_id.apply(lambda s: int(s.split('_')[0]))\n",
    "trlabels['question'] = trlabels.session_id.apply(lambda s: int(s.split('_')[1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf80df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train into 3 groups based on their levels\n",
    "df_0_4 = train[train['level_group'] == '0-4']\n",
    "df_5_12 = train[train['level_group'] == '5-12']\n",
    "df_13_22 = train[train['level_group'] == '13-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35477739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delt_time_def(df):\n",
    "    df.sort_values(by=['session_id', 'elapsed_time'], inplace=True)\n",
    "    df['delt_time'] = df['elapsed_time'].diff(1) # 对于同一个玩家的触发两个相邻的事件时的时间差\n",
    "    df['delt_time'].fillna(0, inplace=True)\n",
    "    df['delt_time'].clip(0, 103000, inplace=True) # 为什么是103000 s = 1716 min = 28 h\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffb2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(train):\n",
    "    # 固定限定ID数据\n",
    "    FIXED_TEXT = ['event_name', 'fqid', 'room_fqid', 'text_fqid', 'page']\n",
    "    # 间隔时间和悬停时间\n",
    "    NUMS = ['delt_time', 'hover_duration']\n",
    "    # 点击事件\n",
    "    EV_NAME = ['checkpoint','observation_click', 'cutscene_click', 'notification_click', 'person_click',\n",
    "               'object_click', 'map_click', 'object_hover']    \n",
    "    # 一个空的DataFeame,最开始只有用户的session_id(去重后的用户会话)作为行索引,因此每一行表示这个用户的一些信息\n",
    "    new_train = pd.DataFrame(index=train['session_id'].unique(), columns=[])\n",
    "    \n",
    "    for c in EV_NAME:\n",
    "        # 统计每一个用户会话某一个点击事件的开始点击次数\n",
    "        new_train['l_ev_name_' + c] = train[ train['event_name'] == c ].groupby(['session_id'])['index'].count()\n",
    "        # 统计每一个用户会话某一个点击事件的相邻点击间隔时间的和\n",
    "        new_train['t_ev_name_' + c] = train[ train['event_name'] == c ].groupby(['session_id'])['delt_time'].sum()\n",
    "    \n",
    "    maska = train['name'] == 'basic' # 事件名称,获得关于base事件的布尔列表\n",
    "    \n",
    "    # 统计每一个用户最后一个发生的用户会话的时间（从开始到最后一个事件）\n",
    "    new_train['finish'] = train[maska].groupby(['session_id'])['elapsed_time'].last(1)\n",
    "    # 统计每个用户在游戏时点击会话事件的总次数\n",
    "    new_train['len'] = train[maska].groupby(['session_id'])['index'].count()\n",
    "    \n",
    "    # 在限定ID数据上的统计，统计每一个用户在限定ID C上的种类数量\n",
    "    for c in FIXED_TEXT:\n",
    "        tmp = train[maska].groupby(['session_id'])[c].agg('nunique')\n",
    "        tmp.name = tmp.name + '_nunique' \n",
    "        new_train = new_train.join(tmp)  # 对DataFrame进行拼接新的一列横着拼接新的列，可以一次性拼接多个\n",
    "    \n",
    "    # 每一个用户在某一个会话任务的间隔会话任务的平均时间和悬停发生时间的平均时间\n",
    "    for c in NUMS:\n",
    "        tmp = train[maska].groupby(['session_id'])[c].agg('mean')\n",
    "        new_train = new_train.join(tmp)\n",
    "    # 为什么使用标准差和平均值来统计时间\n",
    "    # 每一个用户的某一个会话任务的间隔会话任务的标准差和悬停发生时间的标准差\n",
    "    for c in NUMS:\n",
    "        tmp = train[maska].groupby(['session_id'])[c].agg('std')\n",
    "        tmp.name = tmp.name + '_std'\n",
    "        new_train = new_train.join(tmp)\n",
    "    \n",
    "    new_train = new_train.fillna(-1) # 填充缺失值-1\n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e0f07e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train1 shape : (23562, 27)\n",
      "train2 shape : (23561, 27)\n",
      "train3 shape : (22986, 27)\n"
     ]
    }
   ],
   "source": [
    "kol_lvl = (df_0_4.groupby(['session_id'])['level'].agg('nunique') < 5) \n",
    "# 统计每组的level中的值的种类数量，将种类数小于5的组标记为True，也就是只要level数小于5说明这一阶段的都没有通关\n",
    "# print(kol_lvl.shape)\n",
    "# print(kol_lvl)\n",
    "list_session = kol_lvl[kol_lvl].index  \n",
    "# 又因为kol_lvl的行索引就是session_id，因此直接将筛选的布尔矩阵的放进kol_lvl就可以，选择正确的id\n",
    "# print(kol_lvl[kol_lvl])\n",
    "df_0_4 = df_0_4[~ df_0_4['session_id'].isin(list_session) ] \n",
    "# session_id的值不在这个list_session就为False，否则为True，然后取反：获得符合要求的id\n",
    "\n",
    "\n",
    "df_0_4 = delt_time_def(df_0_4)\n",
    "train1 = feature_engineer(df_0_4)\n",
    "\n",
    "print(f\"train1 shape : {train1.shape}\")\n",
    "\n",
    "kol_lvl = (df_5_12.groupby(['session_id'])['level'].agg('nunique') < 8) # 小于8个的id没有通关，不要\n",
    "list_session = kol_lvl[kol_lvl].index  # index返回的是不符合要求的session_id，因为行索引就是session_id\n",
    "\n",
    "df_5_12 = df_5_12[~df_5_12['session_id'].isin(list_session)] # 如果某一列是不正确的id，则取反变成False不选择这一行\n",
    "\n",
    "df_5_12 = delt_time_def(df_5_12)\n",
    "train2 = feature_engineer(df_5_12)\n",
    "\n",
    "print(f\"train2 shape : {train2.shape}\")\n",
    "\n",
    "kol_lvl = (df_13_22.groupby(['session_id'])['level'].agg('nunique') < 10) # 小于10个的id没有通关，不要\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df_13_22 = df_13_22[~ df_13_22['session_id'].isin(list_session) ]\n",
    "\n",
    "df_13_22 = delt_time_def(df_13_22)\n",
    "train3 = feature_engineer(df_13_22)\n",
    "\n",
    "print(f\"train3 shape : {train3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fdab05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain1\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train1' is not defined"
     ]
    }
   ],
   "source": [
    "train1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e133bc49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['event_name', 'name', 'text', 'fqid', 'room_fqid', 'text_fqid'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfqid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroom_fqid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_fqid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m      2\u001b[0m t2 \u001b[38;5;241m=\u001b[39m train2[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_ev_name_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_ev_name_observation_click\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      3\u001b[0m t3 \u001b[38;5;241m=\u001b[39m train3[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_ev_name_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_ev_name_observation_click\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/reshape.py:931\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5856\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5855\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['event_name', 'name', 'text', 'fqid', 'room_fqid', 'text_fqid'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# t2 = train2[['l_ev_name_checkpoint', 'l_ev_name_observation_click']]\n",
    "# t3 = train3[['l_ev_name_checkpoint', 'l_ev_name_observation_click']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec4b8d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will train with 2 2 2 features\n",
      "We will train with 23562 users info\n"
     ]
    }
   ],
   "source": [
    "FEATURES1 = [c for c in t1.columns if c not in ['level_group']] # 由于这里只有NUM类型的数据因此每个level的\n",
    "FEATURES2 = [c for c in t2.columns if c not in ['level_group']]\n",
    "FEATURES3 = [c for c in t3.columns if c not in ['level_group']]\n",
    "print('We will train with', len(FEATURES1), len(FEATURES2), len(FEATURES3) ,'features')\n",
    "ALL_USERS = train1.index.unique()\n",
    "print('We will train with', len(ALL_USERS) ,'users info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c09179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective' : 'binary:logistic',\n",
    "    'eval_metric':'logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 4,\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'tree_method':'hist',\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'use_label_encoder' : False}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7aaa1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiz1, train data shape is (18849, 2), test data shape is (18849,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilysu/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- delt_time\n",
      "- delt_time_std\n",
      "- event_name_nunique\n",
      "- finish\n",
      "- fqid_nunique\n",
      "- ...\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 27 features, but LinearRegression is expecting 2 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train[FEATURES]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m), y_train)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquiz\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train data shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train[FEATURES]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test data shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain data F1 score is \u001b[39m\u001b[38;5;124m\"\u001b[39m, f1_score(y_train, \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest data F1 score is \u001b[39m\u001b[38;5;124m'\u001b[39m, f1_score(y_test, clf\u001b[38;5;241m.\u001b[39mpredict(X_test)))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# SAVE MODEL, PREDICT VALID OOF\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:362\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:345\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    343\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 27 features, but LinearRegression is expecting 2 features as input."
     ]
    }
   ],
   "source": [
    "for t in range(1,19):\n",
    "    # 分别训练不同问题的分类器\n",
    "    if t <= 3:\n",
    "        grp = '0-4'\n",
    "        df = train1\n",
    "        FEATURES = FEATURES1\n",
    "        \n",
    "    elif t <= 13:\n",
    "        grp = '5-12'\n",
    "        df = train2\n",
    "        FEATURES = FEATURES2\n",
    "    \n",
    "    elif t <= 22:\n",
    "        grp = '13-22'\n",
    "        df = train3\n",
    "        FEATURES = FEATURES3\n",
    "    \n",
    "    # TRAIN DATA\n",
    "    train_x = df\n",
    "#     print(train_x.shape, end=',')\n",
    "    train_users = train_x.index.values # 返回train数据集中存在的session_id的列表\n",
    "#     print(train_users, end)\n",
    "    train_y = trlabels.loc[trlabels['question'] == t].set_index('session').loc[train_users] # 选择第t个问题的标签且id在训练数据中的\n",
    "    train_y = train_y['correct']\n",
    "#     print(train_y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2)\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train[FEATURES].astype('float32'), y_train)\n",
    "    print(f'quiz{t}, train data shape is {X_train[FEATURES].shape}, test data shape is {y_train.shape}')\n",
    "    print(\"train data F1 score is \", f1_score(y_train, clf.predict(X_train)))\n",
    "    print(f'test data F1 score is ', f1_score(y_test, clf.predict(X_test)))\n",
    "    # SAVE MODEL, PREDICT VALID OOF\n",
    "    models[f'{grp}_{t}'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ab7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trlabels['session_id'].values)\n",
    "print(trlabels['Question'].values)\n",
    "\n",
    "# print(trlabels['IDQ'].iloc[i].index('_'))\n",
    "#separate session ids into question groups (1-18)\n",
    "#merge train w/ train_labels to get correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge trlabels and train to get Y (correct) and then predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = trlabels.groupby('Question').get_group('q1')\n",
    "\n",
    "IDs = trlabels.groupby('Question').get_group('q1')['session_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = train[~train['session_id'].isin(IDs)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = g1.groupby('level_group').get_group('0-4')\n",
    "# print(len(g1['session_id']))\n",
    "\n",
    "# print(len(group1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test data \n",
    "X = g1\n",
    "Y = group1\n",
    "print(len(X))\n",
    "print(len(Y))\n",
    "# train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.4, random_state = 213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct models\n",
    "#print(X)\n",
    "reg = LinearRegression().fit(train_x, (train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model on test \n",
    "Xtest = test[['session_id', 'music']]\n",
    "\n",
    "print(np.unique(reg.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ddbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy variables "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
